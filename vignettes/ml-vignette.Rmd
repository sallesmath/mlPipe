---
title: "mlPipe-vignette"
author: "Matheus Salles"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ml-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Loading necessary packages:
```{r setup}
library(mlPipe)
library(usethis)
library(devtools)
library(caret)
library(mlbench)
library(caretEnsemble)
```

### Package datasets
Inspect datasets to use mlPipe functions. In this tutorial we're going to use a dataset present in mlbench and caret.

If you want to use other datasets, it is important that you organize your data in a dataframe with samples in rows and characteristics of interest in columns. Also, for the package to work properly, the dependent variable MUST be represented in the last column of your dataframe.
```{r}
data(PimaIndiansDiabetes)

# rename dataset to keep code below generic

my_df <- PimaIndiansDiabetes
head(my_df)
```

--------------

### Package functions
In this tutorial, we're going to start by defining our train and test sets
```{r}
# first of all, you're going to define your training set
my_training_sample <- ml.trainset(my_df, 0.8, seed = 42)
my_training_sample # see your new data

# after that, you can also define your test set
my_test_sample <- ml.testset(my_df, 0.2, seed = 42)
my_test_sample

# It is important that you define a single seed value so that the datasets are compatible
```

Now we're going to do the input file configuration and train our data with a specific machine learning model
```{r}
# Train your data with specific parameters. In this case, we're going to use:
# 80%/20% split between training and test set
# Machine learning algorithm: Linear discriminant analysis (lda)
# Evaluation metric: Accuracy
# Setting a seed: 42
# Resampling method: Cross-validation ("cv")
my_model <- ml.train(train_sample = my_training_sample, algorithm = "lda", metric = "Accuracy", seed = 42, resampling = "cv")
my_model
```

By using another function from our package you can also train multiple models at the same time. The next function wiil generate a list of trained machine learning models.
```{r}
# Train your data with specific parameters. In this case, we're going to use:
# 80%/20% split between training and test set
# Machine learning algorithms: Random Forest (rf), Linear discriminant analysis (lda), Generalized Linear Model (glm), K-Nearest Neighbour (knn)
# Evaluation metric: Accuracy
# Setting a seed: 42
# Resampling method: Repeated cross-validation ("repeatedcv")
my_models <- ml.trainlist(train_sample = my_training_sample, algorithm = c("rf", "lda", "glm", "knn"), metric = "Accuracy", seed = 42, resampling = "cv")
my_models
```

Now that you have a list of trained machine learning models, it's useful to visualize them. In order to do so, you can use the following function
```{r}
# The next functions provide methods for collection, analyzing and visualizing a set of resampling results from a common data set.
results <- resamples(my_models)
summary(results)
# Now we're ready to plot our trained data
# The available graphic options in our package are: bwplot, dotplot, parallelplot and splom. In this tutorial we're going to use dotplot:
ml.graphic(ml_result = results, graphic = "dotplot")
```

Let's assume that LDA was the most accurate model. If you want to get even more insight into your model, you can apply it to a validation dataset. However, for the purposes of this tutorial we will apply our model directly to the test set.

We can run the LDA model directly on the test set and summarize the results in a confusion matrix (using caret functions)
```{r}
predictions <- predict(my_model, my_test_sample)
confusionMatrix(predictions, my_test_sample$diabetes)
```
